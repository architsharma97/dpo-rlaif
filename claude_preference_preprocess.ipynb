{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rlhf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from preference_datasets import get_batch_iterator\n",
    "import transformers\n",
    "cache_dir = '/ebs/.cache/ubuntu/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the ShareGPT dataset...\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shareGPT:  92%|█████████▏| 86709/94145 [00:01<00:00, 63681.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered out [{'from': 'gpt', 'value': 'To assign a class (pos or neg) to the sentence \"A good, good plot and great characters, but poor acting,\" we will use two naive Bayes models: a multinomial naive Bayes model and a binarized naive Bayes model. Both models will use add-1 smoothing.\\n\\nFirst, we need to compute the likelihoods for each word in each class. For the multinomial model, we will use the frequency of each word in each class. For the binarized model, we will use the presence or absence of each word in each class.\\n\\nMultinomial model:\\n\\n| Word | Pos freq | Neg freq |\\n| --- | --- | --- |\\n| good | 6 | 1 |\\n| poor | 0 | 8 |\\n| great | 5 | 2 |\\n\\nBinarized model:\\n\\n| Word | Pos freq | Neg freq |\\n| --- | --- | --- |\\n| good | 1 | 1 |\\n| poor | 0 | 1 |\\n| great | 1 | 1 |\\n\\nNext, we need to compute the prior probabilities for each class. In this case, we have two positive and three negative documents, so the priors are:\\n\\nP(pos) = 2/5 = 0.4\\nP(neg) = 3/5 = 0.6\\n\\nNow we can compute the likelihood for the sentence using each model. For the multinomial model, we simply multiply the likelihoods for each word in the sentence. For the binarized model, we use the presence or absence of each word in the sentence.\\n\\nMultinomial model:\\n\\nP(pos | sentence) = P(pos) \\\\* P(good | pos)^2 \\\\* P(great | pos) \\\\* P(poor | pos)\\n= 0.4 \\\\* (7/14)^2 \\\\* 5/14 \\\\* 1/14\\n= 0.000329\\n\\nP(neg | sentence) = P(neg) \\\\* P(good | neg)^2 \\\\* P(great | neg) \\\\* P(poor | neg)\\n= 0.6 \\\\* 1/16 \\\\* 2/16 \\\\* (9/16)^2\\n= 0.000146\\n\\nBinarized model:\\n\\nP(pos | sentence) = P(pos) \\\\* P(good | pos) \\\\* P(great | pos) \\\\* P(poor | pos)\\n= 0.4 \\\\* 1/2 \\\\* 1/2 \\\\* 1/4\\n= 0.025\\n\\nP(neg | sentence) = P(neg) \\\\* P(good | neg) \\\\* P(great | neg) \\\\* P(poor | neg)\\n= 0.6 \\\\* 1/3 \\\\* 1/3 \\\\* 3/4\\n= 0.075\\n\\nTherefore, the multinomial model predicts a negative class for the sentence, while the binarized model predicts a positive class. The two models disagree.'}, {'from': 'human', 'value': 'Write out the equation for trigram probability estimation (modifying Eq. 3.11).\\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\\non page 34.'}, {'from': 'gpt', 'value': 'The trigram probability estimation formula, modified from Eq. 3.11, is:\\n\\nP(wi | wi-2, wi-1) = C(wi-2, wi-1, wi) / C(wi-2, wi-1)\\n\\nwhere wi-2, wi-1, and wi are the three words in the trigram, C(wi-2, wi-1, wi) is the count of the trigram in the corpus, and C(wi-2, wi-1) is the count of the bigram (wi-2, wi-1) in the corpus.\\n\\nFor the I am Sam corpus on page 34, the non-zero trigram probabilities are:\\n\\nP(like | I, would) = 1/1 = 1.000\\nP(good | I, like) = 1/1 = 1.000\\nP(the | like, to) = 1/1 = 1.000\\nP(movie | to, watch) = 1/1 = 1.000\\nP(would | I, like) = 1/1 = 1.000\\nP(to | like, would) = 1/1 = 1.000\\nP(watch | would, like) = 1/1 = 1.000\\nP(I | would, like) = 1/2 = 0.500\\nP(to | I, watch) = 1/1 = 1.000\\nP(like | watch, the) = 1/1 = 1.000\\nP(it | watch, the) = 1/1 = 1.000\\nP(was | the, good) = 1/1 = 1.000\\nP(good | the, movie) = 1/1 = 1.000\\nP(the | movie, was) = 1/1 = 1.000\\nP(a | was, good) = 1/1 = 1.000\\nP(lot | good, it) = 1/1 = 1.000\\nP(of | it, a) = 1/1 = 1.000'}, {'from': 'human', 'value': 'Calculate the probability of the sentence i want chinese food. Give two\\nprobabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on\\npage 36, and another using the add-1 smoothed table in Fig. 3.7. Assume the\\nadditional add-1 smoothed probabilities P(i|~~) = 0.19 and P(~~|food) =\\n0.40'}, {'from': 'gpt', 'value': 'Using Fig. 3.2 and the useful probabilities just below it on page 36, we have:\\n\\nP(\"i want chinese food\") = P(i) \\\\* P(want|i) \\\\* P(chinese|want) \\\\* P(food|chinese)\\n\\n= 0.09 \\\\* 0.000016 \\\\* 0.00026 \\\\* 0.0011\\n\\n= 4.342 × 10^-12\\n\\nUsing the add-1 smoothed table in Fig. 3.7, we have:\\n\\nP(\"i want chinese food\") = P(i) \\\\* P(want|i) \\\\* P(chinese|want) \\\\* P(food|chinese)\\n\\n= (4+1)/(47+25) \\\\* (1+1)/(4+25) \\\\* (1+1)/(3+25) \\\\* (2+1)/(35+25)\\n\\n= 0.00001262\\n\\nWe also need to consider the additional add-1 smoothed probabilities:\\n\\nP(i|<s>) = 0.19\\nP(</s>|food) = 0.40\\n\\nTherefore, the overall probability of the sentence is:\\n\\nP(\"i want chinese food\") = P(i|<s>) \\\\* P(\"i want chinese food\") \\\\* P(</s>|food)\\n\\n= 0.19 \\\\* 0.00001262 \\\\* 0.40\\n\\n= 9.552 × 10^-7'}, {'from': 'human', 'value': 'Which of the two probabilities you computed in the previous exercise is higher,\\nunsmoothed or smoothed? Explain why.'}, {'from': 'gpt', 'value': 'The probability computed using the smoothed probabilities is higher than the probability computed using the unsmoothed probabilities. This is because smoothing adds some probability mass to unseen n-grams, which helps avoid zero probabilities and provides a more robust estimate of probabilities. In the unsmoothed case, the probability of the sentence is the product of the probabilities of the individual words in the sentence, and since some of these words are not present in the training corpus, their probabilities are zero, leading to a very small overall probability for the sentence. In the smoothed case, the addition of a small count to each n-gram ensures that no n-gram has zero probability, and hence the overall probability of the sentence is larger.'}, {'from': 'human', 'value': 'We are given the following corpus, modified from the one in the chapter:\\n ~~I am Sam~~ \\n ~~Sam I am~~ \\n ~~I am Sam~~ \\n ~~I do not like green eggs and Sam~~ \\nUsing a bigram language model with add-one smoothing, what is P(Sam |\\nam)? Include  ~~and~~  in your counts just like any other token'}, {'from': 'gpt', 'value': 'To calculate the probability of the word \"Sam\" given the previous word \"am\" using a bigram model with add-one smoothing, we first need to count the occurrences of the bigram \"am Sam\" and the unigram \"am\" in the training corpus.\\n\\nThe bigram count for \"am Sam\" is 2, since it occurs twice in the corpus: \"I am Sam\" and \"I am Sam\". The unigram count for \"am\" is 3, since it occurs three times: \"I am\", \"Sam I\", and \"and Sam\".\\n\\nWe also need to count the total number of bigrams that begin with the word \"am\", including the special start-of-sentence token \"<s>\". In this case, there is only one such bigram, \"<s> am\".\\n\\nFinally, we add one to each bigram count and the number of possible bigrams to account for add-one smoothing. The smoothed probability of \"Sam\" given \"am\" is therefore:\\n\\nP(Sam | am) = (count(am Sam) + 1) / (count(am) + V)\\n\\nwhere V is the size of the vocabulary, including the start and end-of-sentence tokens. In this case, V = 7, since there are six unique words in the corpus plus the two special tokens.\\n\\nThus, we have:\\n\\nP(Sam | am) = (2 + 1) / (3 + 7) = 3/10 = 0.3\\n\\nSo the probability of \"Sam\" given \"am\" is 0.3, using a bigram language model with add-one smoothing.'}]\n",
      "--------------------------------------------------------------------------------\n",
      "filtered out [{'from': 'gpt', 'value': 'To calculate the probability of the word \"Sam\" given the previous word \"am\" using a bigram model with add-one smoothing, we first need to count the occurrences of the bigram \"am Sam\" and the unigram \"am\" in the training corpus.\\n\\nThe bigram count for \"am Sam\" is 2, since it occurs twice in the corpus: \"I am Sam\" and \"I am Sam\". The unigram count for \"am\" is 3, since it occurs three times: \"I am\", \"Sam I\", and \"and Sam\".\\n\\nWe also need to count the total number of bigrams that begin with the word \"am\", including the special start-of-sentence token \"<s>\". In this case, there is only one such bigram, \"<s> am\".\\n\\nFinally, we add one to each bigram count and the number of possible bigrams to account for add-one smoothing. The smoothed probability of \"Sam\" given \"am\" is therefore:\\n\\nP(Sam | am) = (count(am Sam) + 1) / (count(am) + V)\\n\\nwhere V is the size of the vocabulary, including the start and end-of-sentence tokens. In this case, V = 7, since there are six unique words in the corpus plus the two special tokens.\\n\\nThus, we have:\\n\\nP(Sam | am) = (2 + 1) / (3 + 7) = 3/10 = 0.3\\n\\nSo the probability of \"Sam\" given \"am\" is 0.3, using a bigram language model with add-one smoothing.'}, {'from': 'human', 'value': 'Suppose we didn’t use the end-symbol . Train an unsmoothed bigram\\ngrammar on the following training corpus without using the end-symbol :\\n ~~a b\\n ~~b b\\n ~~b a\\n ~~a a\\nDemonstrate that your bigram model does not assign a single probability dis\\x02tribution across all sentence lengths by showing that the sum of the probability\\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\\nis also 1.0.~~~~~~~~'}, {'from': 'gpt', 'value': 'Without using the end-symbol </s>, the bigram model assigns probabilities only to pairs of adjacent words in the training corpus. Therefore, the sum of the probabilities of all possible 2-word sentences and 3-word sentences over the alphabet {a,b} is not necessarily equal to 1.0.\\n\\nFor the given training corpus, the possible 2-word sentences over the alphabet {a,b} are:\\n\\n* <s> a\\n* <s> b\\n* a b\\n* b b\\n* b a\\n* a a\\n\\nThe probability of each of these sentences is:\\n\\n* P(a|<s>) x P(b|a) = 1/2 x 1 = 1/2\\n* P(b|<s>) x P(b|b) = 1/2 x 1/2 = 1/4\\n* P(a|<s>) x P(a|a) = 1/2 x 1/2 = 1/4\\n* P(b|b) x P(a|b) = 1/2 x 1 = 1/2\\n* P(a|b) x P(b|a) = 1/2 x 1 = 1/2\\n* P(a|a) x P(a|a) = 1/2 x 1/2 = 1/4\\n\\nThe sum of these probabilities is 7/4, which is greater than 1.0.\\n\\nThe possible 3-word sentences over the alphabet {a,b} are:\\n\\n* <s> a b\\n* <s> b b\\n* <s> b a\\n* <s> a a\\n* a b b\\n* b b a\\n* b a a\\n* a a b\\n\\nThe probability of each of these sentences is:\\n\\n* P(a|<s>) x P(b|a) x P(</s>|b) = 1/2 x 1 x 0 = 0\\n* P(b|<s>) x P(b|b) x P(</s>|b) = 1/2 x 1/2 x 0 = 0\\n* P(b|<s>) x P(a|b) x P(</s>|a) = 1/2 x 1 x 0 = 0\\n* P(a|<s>) x P(a|a) x P(</s>|a) = 1/2 x 1/2 x 0 = 0\\n* P(a|a) x P(b|a) x P(b|b) = 1/2 x 1 x 1/2 = 1/4\\n* P(b|b) x P(b|b) x P(a|b) = 1/2 x 1/2 x 1 = 1/4\\n* P(b|b) x P(a|b) x P(a|a) = 1/2 x 1 x 1/2 = 1/4\\n* P(a|a) x P(a|a) x P(b|a) = 1/2 x 1/2 x 1 = 1/4\\n\\nThe sum of these probabilities is also 1.0. Therefore, the bigram model does not assign a single probability distribution across all sentence lengths.'}, {'from': 'human', 'value': 'Suppose we train a trigram language model with add-one smoothing on a\\ngiven corpus. The corpus contains V word types. Express a formula for esti\\x02mating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\\nin terms of various n-gram counts and V. Use the notation c(w1,w2,w3) to\\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\\nso on for bigrams and unigrams.'}, {'from': 'gpt', 'value': 'The formula for estimating trigram probabilities with add-one smoothing is:\\n\\nP(w3|w1,w2) = (c(w1,w2,w3) + 1) / (c(w1,w2) + V)\\n\\nwhere:\\n\\n* c(w1,w2,w3) is the count of the trigram (w1,w2,w3) in the corpus.\\n* c(w1,w2) is the count of the bigram (w1,w2) in the corpus.\\n* V is the vocabulary size, i.e., the number of unique word types in the corpus.\\n\\nThe numerator adds 1 to the trigram count to account for possible unseen trigrams and ensure non-zero probabilities. The denominator adds V to the bigram count to account for the additional possible next words (including unseen words) and ensure that the probabilities sum to 1 over the entire vocabulary.'}, {'from': 'human', 'value': 'Write a program to compute unsmoothed unigrams and bigrams.'}, {'from': 'gpt', 'value': 'Here is an example implementation of a program to compute unsmoothed unigrams and bigrams in Python:\\n```python\\nimport re\\nfrom collections import defaultdict\\n\\n# read in text file\\nwith open(\\'corpus.txt\\') as f:\\n    text = f.read()\\n\\n# split text into sentences\\nsentences = re.split(\\'[.!?]\\', text)\\n\\n# compute unigrams\\nunigrams = defaultdict(int)\\nfor sentence in sentences:\\n    words = sentence.strip().split()\\n    for word in words:\\n        unigrams[word] += 1\\n\\n# compute bigrams\\nbigrams = defaultdict(int)\\nfor sentence in sentences:\\n    words = sentence.strip().split()\\n    for i in range(len(words) - 1):\\n        bigram = (words[i], words[i+1])\\n        bigrams[bigram] += 1\\n\\n# print unigrams and bigrams\\nprint(\"Unigrams:\")\\nfor word, count in unigrams.items():\\n    print(f\"{word}: {count}\")\\n\\nprint(\"Bigrams:\")\\nfor bigram, count in bigrams.items():\\n    print(f\"{bigram[0]} {bigram[1]}: {count}\")\\n```\\nThis program reads in a text file called `corpus.txt`, splits the text into sentences, and then computes unigrams and bigrams for each sentence. The resulting unigrams and bigrams are stored in `defaultdicts` for ease of use. The program then prints out the unigrams and bigrams, with each line showing the word(s) and their corresponding count.'}, {'from': 'human', 'value': 'Add an option to your program to generate random sentences.'}, {'from': 'gpt', 'value': \"Here's an example Python program that generates random sentences using a bigram language model:\\n```python\\nimport random\\n\\n# Define a bigram language model\\nbigrams = {}\\n\\n# Load the corpus and count bigrams\\nwith open('corpus.txt', 'r') as f:\\n    words = f.read().split()\\n    for i in range(len(words)-1):\\n        w1, w2 = words[i], words[i+1]\\n        if w1 not in bigrams:\\n            bigrams[w1] = {}\\n        if w2 not in bigrams[w1]:\\n            bigrams[w1][w2] = 0\\n        bigrams[w1][w2] += 1\\n\\n# Generate random sentences\\nfor i in range(5):\\n    # Choose a random starting word\\n    sentence = [random.choice(list(bigrams.keys()))]\\n\\n    # Generate the rest of the sentence\\n    while True:\\n        prev_word = sentence[-1]\\n        if prev_word not in bigrams:\\n            break\\n        next_word = random.choice(list(bigrams[prev_word].keys()))\\n        sentence.append(next_word)\\n        if next_word.endswith('.'):\\n            break\\n\\n    # Print the sentence\\n    print(' '.join(sentence))\\n```\\nThis program reads in a corpus from a file called `corpus.txt`, which should contain whitespace-separated words. It then builds a bigram language model by counting the frequency of each bigram in the corpus. Finally, it generates five random sentences by starting with a random word and choosing subsequent words based on the bigram probabilities. The program stops generating the sentence when it reaches a period.\"}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shareGPT: 100%|██████████| 94145/94145 [00:01<00:00, 55683.37it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3159 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a dataset with 71894 prompts from ShareGPT\n",
      "Finished generating 1 epochs on combined split\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('huggyllama/llama-7b', cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "prompt_iterator = get_batch_iterator(['sharegpt'], tokenizer=tokenizer, split='combined', batch_size=1, sft_mode=True,\n",
    "                                     seed=0, n_epochs=1, cache_dir=cache_dir, shuffle=False,\n",
    "                                     max_prompt_length=256, max_length=1024, data_fraction=1.0, num_turns=1)\n",
    "\n",
    "chatgpt_instruction_truncoutput_pair = []\n",
    "single_turn_instructions = []\n",
    "for batch in prompt_iterator:\n",
    "    instruction = batch['prompt'][0]\n",
    "    if instruction.startswith('Assistant:'):\n",
    "        continue\n",
    "    instruction = instruction[len('Human: '):-len('\\n\\nAssistant: ')]\n",
    "    prompt_token_count = batch['prompt_input_ids'][0].shape[0]\n",
    "    output = tokenizer.decode(batch['chosen_input_ids'][0][prompt_token_count:], skip_special_tokens=True)\n",
    "    chatgpt_instruction_truncoutput_pair.append({'instruction': instruction,\n",
    "                                                 'output': output})\n",
    "    single_turn_instructions.append(instruction)\n",
    "\n",
    "# print(instructions[0] == chatgpt_instruction_truncoutput_pair[0]['instruction'])\n",
    "# chatgpt_instruction_truncoutput_pair[0]\n",
    "# print(len(instructions), len(chatgpt_instruction_truncoutput_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llama_samples_from_dir(sample_folder):\n",
    "    sft_instruction_truncoutput_pair = []\n",
    "    sft_instructions = []\n",
    "    for sample_file in os.listdir(sample_folder):\n",
    "        sft_outputs = json.load(open(os.path.join(sample_folder, sample_file), 'r'))\n",
    "        for instruction, sft_output in sft_outputs.items():\n",
    "            instruction_trimmed = instruction[len('Human: '):-len('\\n\\nAssistant: ')] # some prompts will be processed incorrectly, but only the single-turn ones matter\n",
    "            sft_instruction_truncoutput_pair.append({'instruction': instruction_trimmed,\n",
    "                                                    'output': sft_output[0]})\n",
    "            sft_instructions.append(instruction_trimmed)\n",
    "    return sft_instruction_truncoutput_pair, sft_instructions\n",
    "\n",
    "def match_instruction_outputs(instruction_set_1, instruct_out_1, instruction_set_2, instruct_out_2):\n",
    "    matched_instruction_outputs = []\n",
    "    for idx, instruction in enumerate(instruction_set_1):\n",
    "        if instruction in instruction_set_2 and instruction in single_turn_instructions:\n",
    "            matched_instruction_outputs.append({'instruction': instruction,\n",
    "                                                'output_1': instruct_out_1[idx]['output'],\n",
    "                                                'output_2': instruct_out_2[instruction_set_2.index(instruction)]['output']})\n",
    "    return matched_instruction_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/ebs/.cache/ubuntu/sharegpt2turn_llama7b_sft_maxlen1024_2023-09-11_21-52-36_584206/step-10000/'\n",
    "model1_name = 'temp2.5'\n",
    "model2_name = 'chatgpt'\n",
    "max_comparisons = 200\n",
    "\n",
    "sft_instruction_truncoutput_pair, sft_instructions = process_llama_samples_from_dir(os.path.join(base_dir, f'sharegpt2turn_noeos_maxlen1024_{model1_name}'))\n",
    "\n",
    "if model2_name == 'chatgpt':\n",
    "    matched_instruction_outputs = match_instruction_outputs(sft_instructions, sft_instruction_truncoutput_pair, single_turn_instructions, chatgpt_instruction_truncoutput_pair)\n",
    "else:\n",
    "    sft_instruction_truncoutput_pair_2, sft_instructions_2 = process_llama_samples_from_dir(os.path.join(base_dir, f'sharegpt2turn_noeos_maxlen1024_{model2_name}'))\n",
    "    matched_instruction_outputs = match_instruction_outputs(sft_instructions, sft_instruction_truncoutput_pair, sft_instructions_2, sft_instruction_truncoutput_pair_2)\n",
    "\n",
    "comparison_folder = os.path.join(base_dir, 'comparisons', f'{model1_name}_vs_{model2_name}')\n",
    "os.makedirs(comparison_folder, exist_ok=True)\n",
    "\n",
    "model1_outputs = []\n",
    "model2_outputs = []\n",
    "for idx, matched_instruction_output in enumerate(matched_instruction_outputs):\n",
    "    model1_outputs.append({'instruction': matched_instruction_output['instruction'],\n",
    "                           'output': matched_instruction_output['output_1'],\n",
    "                           \"generator\": model1_name,})\n",
    "    model2_outputs.append({'instruction': matched_instruction_output['instruction'],\n",
    "                           'output': matched_instruction_output['output_2'],\n",
    "                           \"generator\": model2_name,})\n",
    "    if idx > max_comparisons:\n",
    "        break\n",
    "\n",
    "with open(os.path.join(comparison_folder, model1_name + '.json'), 'w') as f:\n",
    "    json.dump(model1_outputs, f, indent=4)\n",
    "\n",
    "with open(os.path.join(comparison_folder, model2_name + '.json'), 'w') as f:\n",
    "    json.dump(model2_outputs, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
